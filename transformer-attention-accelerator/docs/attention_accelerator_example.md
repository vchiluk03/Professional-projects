# Example: Why This Design Matters in Real SoCs

Let‚Äôs understand the motivation behind this design with a simple example.

---

## üß† The Attention Problem

Imagine you‚Äôre reading a sentence:
> ‚ÄúThe cat sat on the mat because it was tired.‚Äù

When the Transformer processes the word ‚Äúit‚Äù, it must decide whether ‚Äúit‚Äù refers to the **cat** or the **mat**.  
To do this, it calculates how strongly ‚Äúit‚Äù relates to every other word ‚Äî that‚Äôs the **attention mechanism**.

It builds three key matrices:
- **Q (Query):** Represents the current word (‚Äúit‚Äù)
- **K (Key):** Represents all other words
- **V (Value):** Contains the meaning of each word

![Query and Key generation](../docs/query_key_generation.png)

Then it computes:
> **S = Q √ó K·µÄ** ‚Üí attention scores  
> **Z = softmax(S) √ó V** ‚Üí weighted output

![Score matrix multiplication](../docs/score_matrix_multiplication.png)

---

## ‚öôÔ∏è The Problem on CPUs and GPUs

For a sentence with 512 tokens:
- Q, K, V are 512√ó512 matrices  
- That‚Äôs over **134 million multiplications** just to find attention scores  
- And Transformers repeat this across **dozens of layers**

CPUs and GPUs can compute this but waste energy moving data between memory and compute units ‚Äî fine for data centers, not for **real-time or on-device AI**.

---

## ‚ö° What the Hardware Accelerator Does Better

Your design focuses **only on this math** ‚Äî nothing else.  
It uses:
1. A **15-state FSM** to control every operation precisely  
2. **Local SRAMs** for Input, Weight, Result, and Scratchpad data  
3. A **Multiply-Accumulate unit** that runs continuously  
4. Smart **transposed memory storage** for efficient V access during `Z = S√óV`

üí° **Result:**  
- No wasted cycles  
- No cache misses  
- Lower energy per operation  
- Deterministic latency

---

## üß© In a Real SoC

Inside a modern SoC (e.g., Qualcomm Snapdragon or Apple M-series):
- The **CPU** sends data to your accelerator  
- Your block performs the Q, K, V, S, Z computations  
- Results go back to system memory or the next AI pipeline stage  
- The **NPU or AI engine** repeats this across multiple attention heads and layers

This accelerator becomes a **building block** for Transformer-based workloads:
- Speech recognition  
- On-device translation  
- Vision Transformers (object detection)  
- LLM inference acceleration  

---

## üîö Summary

Transformers are huge relational calculators.  
Your design builds a **small, specialized calculator** that performs just the attention math ‚Äî perfectly, efficiently, and at hardware speed.  
That‚Äôs why such accelerators are becoming a key part of **modern AI SoCs**.
